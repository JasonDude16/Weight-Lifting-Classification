---
title: "Weight Lifting Classification"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary 
In this analysis I created several machine learning models with the goal of classifying several variations of a weight lifting exercise (e.g., barbell curl). In some instances, the exercise was performed with good form, and in other instances the the exercise was intentionally performed with poor form (e.g., swinging hips). The model was used to differentiate between 5 variations of the exercise using 3 accelerometers that were placed on the arm, upper arm, and waist (located anteriorly) of 6 participants. 

An overview of the dataset and where it was collected from can be found [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). 

```{r include=FALSE, cache=TRUE}
mod_treebag <- readRDS("./objects/bagging.rds")
mod_gbm <- readRDS("./objects/boosting.rds")
mod_ensemble <- readRDS("./objects/ensemble.rds")
mod_nb <- readRDS("./objects/nb.rds")
mod_rf <- readRDS("./objects/rf.rds")
```

# Importing and Viewing
The following packages are needed to reproduce the code in this document.

```{r message=FALSE, warning=FALSE}
set.seed(123)
library(caret) 
library(ggplot2)
library(dplyr)
library(rattle) 
library(plyr) 
library(randomForest)
```

* The training set can be downloaded [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

* The testing set can be downloaded [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

```{r cache=TRUE}
training <- read.csv("./data/pml-training.csv")
testing <- read.csv("./data/pml-testing.csv")
```

Below are the main functions I used to get a feel for the data. I didn't print the results to save space. The dataset consists of `r nrow(training)` rows and `r ncol(training)` columns.

```{r eval=FALSE}
str(training)
summary(training)
colSums(is.na(training))
training[,sapply(training, is.character)]
```

```{r echo=FALSE}
rmarkdown::paged_table(training[1:100,], options = list(rows.print = 10))
```

# Data Cleaning
For data cleaning/processing, I converted the outcome variable to a numeric factor, then removed columns with NAs and empty character values, `""`. This removed nearly half of the predictor variables. This is because many features were aggregated/summary features; for example, `max_roll_belt`, which consisted only of the maximum roll belt value. These aggregate features could be very useful for more efficient analyses with less variables, but for now I'll leave them out. Additionally, the first few columns were removed which consisted of an indexing variable, a user name variable, and time stamps. 

```{r}
# CREATE DUMMY VARS FOR OUTCOME
training$classe <- mapvalues(training$classe, c("A", "B", "C", "D", "E"), 0:4)
training$classe <- as.factor(training$classe)

# REMOVE NA COLUMNS 
NAs <- unlist(which(colSums(is.na(training)) > 0))
training <- training[-NAs]

# REMOVE CHAR NA COLUMNS 
char_NAs <- unlist(which(sapply(training, is.character)))
training <- training[-char_NAs]

# REMOVE INDEX AND TIMESTAMP COLUMNS
training <- training[-1:-4]
```

After the dataset was cleaned, the variance of the remained variables was assessed. If the variance was near zero, I would've removed the variables (if it seemed appropriate); but none of the remaining variables had near zero variance. 
```{r}
# ASSESS VARIANCE OF REMAINING VARIABLES
nearZeroVar(training)
```

The cleaned dataset consists of `r nrow(training)` rows and `r ncol(training)` columns. Except for the outcome variable, all variables were either integer or numeric.

```{r echo=FALSE}
rmarkdown::paged_table(training[1:100,], options = list(rows.print = 10))
```

# Exploratory Analysis
Below is a shiny app I created to visualize the data. Histograms, boxplots, and scatter plots can be created, data can be grouped or ungrouped, and the scaling can be adjusted. The app initially computed kmeans classification accuracy for the `X` and `Y` variable, but unfortunately that feature did not work once the app was uploaded to the shiny server. The app is certainly not perfect (and honestly not that practical), but I thought it was a cool idea.

<iframe src ="https://jason-dude.shinyapps.io/Visualizing-Weight-Classification/" height=700px width=1000 />

# Modeling 
Now we're on to the fun stuff, modeling! In the Johns Hopkins University  [Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science) I was exposed to model implementation with the `caret` package. I decided to try out five different models: Treebag, Gradient Boosting, Naive Bayes, Ensemble, and Random Forest. The ensemble model used the combination of the Treebag and Gradient Boosting models. 

### Treebag Model

```{r eval=FALSE}
mod_treebag <- train(classe ~ ., 
                     data = training, 
                     preProcess = "pca", 
                     method = "treebag")
```

```{r cm_tb, cache=TRUE}
confusionMatrix(predict(mod_treebag, training), training$classe)
```

### Gradient Boosting Model 

```{r eval=FALSE}
trainctrl <- trainControl(verboseIter = TRUE, number = 5)
mod_gbm <- train(classe  ~ ., 
                 data = training,
                 preProcess = "pca",
                 method = "gbm",
                 verbose = TRUE,
                 trControl = trainctrl)
```

```{r cm_gbm, cache=TRUE}
confusionMatrix(predict(mod_gbm, training), training$classe)
```

### Naive Bayes Model

```{r eval=FALSE}
trainctrl <- trainControl(number = 5, verboseIter = TRUE)
mod_nb <- train(classe  ~ ., 
                data = training,
                preProcess = "pca",
                method = "nb",
                trControl = trainctrl)
```

```{r cm_nb, cache=TRUE, warning=FALSE}
confusionMatrix(predict(mod_nb, training), training$classe)
```

### Ensemble Model

```{r all_preds, cache=TRUE}
boost_pred <- predict(mod_gbm, training)
bagging_pred <- predict(mod_treebag, training)
all_preds <- data.frame(boost_pred, bagging_pred, classe = training$classe)
```

```{r eval=FALSE}
mod_ensemble <- train(classe ~ ., data = all_preds, method = "treebag")
```

```{r cm_ensemble, cache=TRUE}
confusionMatrix(predict(mod_ensemble, training), training$classe)
```

### Random Forest

```{r eval=FALSE}
mod_rf <- randomForest(classe ~. , data = training, do.trace = TRUE)
```

```{r cm_rf, cache=TRUE}
confusionMatrix(predict(mod_rf, training), training$classe)
```

### Summary of Models
The `treebag`, `gradient boosting`, and `naive bayes` models were all pre-processed using principal components analysis. The `ensemble` model was as well, because the model combined the `gradient boosting` and `treebag` models which both implemented PCA. The models were left to their default settings, except the `boosting` and `naive bayes` were both set to `number = 5`. This was done because the models were taking awhile to finish, so I lowered the number to speed up the process. As you can see in the printout summaries the `Random Forest` model had perfect accuracy. 

> Note: Pre-processing with PCA in the `caret` package centers and scales the features. 

# Results 
I implemented the `Random Forest` model for the test set because, after doing a little more research, it seems that this model performs well with noisy data. This information, combined with the 100% accuracy on the training set, made it the sensible choice.

```{r}
predictions <- predict(mod_rf, testing)
factor(predictions, labels = c("A", "B", "C", "D", "E"))
```

The testing data did not include the `classe` because this dataset was used as a quiz for the **Practical Machine Learning** on Coursera, but the model achieved 100% accuracy on this small sample! 

